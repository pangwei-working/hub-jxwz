# 第五周
1、 申请deepseek的api，https://platform.deepseek.com/usage， 使用openai 库调用云端大模型。

​		openai调用云端deepseek.py

2、 本地安装ollama工具，尝试在本地部署一个qwen3:0.6b模型

​		![WX20250912-144216@2x.png](WX20250912-144216%402x.png)

3、 阅读政企问答项目的代码，自己描述RAG的实现流程，写为文档。

        rag在我的理解当中，简单来讲就是将企业已有的处理问题的相关文档，以及各种描述性或者政策性文件
    等等一切可能需要到的文档作为一个知识库，知识库的作用呢就是作为用户提问的提示词去给到大模型使用。
        完整的RAG应用流程主要包含两个阶段：
        ·数据准备阶段：数据提取——>文本分割——>向量化（embedding）——>数据入库
        ·应用阶段：用户提问——>数据检索（召回）——>注入Prompt——>LLM生成答案
    
    针对这个政企项目：
        1.数据获取：由客户提供，或者和客户沟通从他们各种历史数据库中清洗数据来提取项目需要的原始数据,
                以上都不行的话，那只有去爬虫爬相关的公开数据了。
        2.文本分割：选择合适的分割方式，一个好的分割方式可以大大提高搜索时候的命中率，针对这个项目中的文档类型，使用
                字符分割器设置好合适chunkSize和overlapSize应该就有相对不错的效果
        3.文本向量化：选择合适的向量化方式，Bert 进行向量化 至于选择哪种https://huggingface.co/spaces/mteb/leaderboard_legacy
                可以参考这个网站
        4.数据入库：存储数据到数据库中，方便后续查询 数据库选择es
        5.用户提问：获取用户输入，并预处理，比如去除标点符号去除，停用词，然后转换为向量。
        6.数据检索（召回）：使用向量化后的文本进行向量检索，并返回最相似的文本，为了更好的检索效果可以实现多路召回。
        7.注入Prompt：添加prompt，将返回的文档作为动态提示词去丢给大模型
        8.LLM生成答案：使用大模型进行文本生成，生成结果作为答案。