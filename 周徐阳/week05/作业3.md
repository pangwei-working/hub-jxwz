# 政企问答RAG系统实现流程分析

## 1. 系统架构概览

这个政企问答系统采用了经典的RAG（Retrieval-Augmented Generation）架构，主要包含以下核心组件：

### 1.1 技术栈
- **Web框架**: FastAPI - 提供RESTful API服务
- **向量检索**: Elasticsearch - 存储文档向量并支持混合检索
- **关系数据库**: SQLite - 存储知识库和文档元数据
- **文本向量化**: BGE-small-zh-v1.5 - 中文文本embedding模型
- **重排序**: BGE-reranker-base - 对检索结果进行重排序（可选）
- **大语言模型**: GLM-4-air - 用于最终的答案生成
- **文档处理**: pdfplumber - PDF文档解析

### 1.2 数据存储架构

系统采用分层存储架构：

**关系数据库层（SQLite）**
- `knowledge_database` 表：存储知识库基本信息
- `knowledge_document` 表：存储文档元数据

**向量检索层（Elasticsearch）**  
- `document_meta` 索引：存储文档摘要和全文内容
- `chunk_info` 索引：存储文档分块、向量表示和页面信息

## 2. 核心实现流程

### 2.1 文档处理与入库流程

#### 步骤1：文档上传接口
```python
@app.post("/v1/document")
async def add_document(knowledge_id, title, category, file, background_tasks)
```

**主要处理逻辑：**
1. 接收用户上传的文档文件
2. 在数据库中创建文档记录，获得`document_id`
3. 将文件保存到本地存储目录
4. 启动后台任务进行文档内容提取和向量化

#### 步骤2：文档内容提取（PDF处理）
```python
def _extract_pdf_content(self, knowledge_id, document_id, title, file_path)
```

**处理过程：**
1. 使用pdfplumber打开PDF文件
2. 逐页提取文本内容
3. 前4页内容作为文档摘要
4. 对每页内容进行向量化编码
5. 将页级内容存储到ES的chunk_info索引

#### 步骤3：文档分块处理
```python
def split_text_with_overlap(text, chunk_size, chunk_overlap)
```

**分块策略：**
- 固定长度分块：默认256字符
- 重叠处理：默认20字符重叠，保证语义连续性
- 每个chunk独立编码并存储向量表示

#### 步骤4：向量化存储
对每个文本块进行embedding：
```python
def get_embedding(self, text) -> np.ndarray:
    return EMBEDDING_MODEL_PARAMS["embedding_model"].encode(text, normalize_embeddings=True)
```

### 2.2 问答检索流程

#### 步骤1：问答接口调用
```python  
@app.post("/chat")
def chat(req: RAGRequest) -> RAGResponse
```

#### 步骤2：混合检索策略
```python
def query_document(self, query: str, knowledge_id: int) -> List[str]
```

**检索包含两个并行过程：**

**全文检索（BM25）：**
- 基于Elasticsearch的match查询
- 使用ik分词器进行中文分词
- 返回相关性最高的50个候选块

**向量检索（KNN）：**
- 对查询语句进行向量编码
- 在ES中执行KNN向量相似性搜索
- 返回语义最相似的50个候选块

#### 步骤3：结果融合（RRF - Reciprocal Rank Fusion）
```python
# RRF融合算法实现
k = 60
fusion_score = {}
for idx, record in enumerate(search_results):
    _id = record["_id"]
    fusion_score[_id] = fusion_score.get(_id, 0) + 1 / (idx + k)
```

**融合策略：**
- 使用RRF算法融合两种检索结果
- 每个文档的最终得分 = BM25得分 + 向量检索得分
- 选择Top-K个最相关的文档块

#### 步骤4：重排序（可选）
```python
def get_rank(self, text_pair) -> np.ndarray:
    # 使用BGE-reranker模型对查询-文档对进行精确打分
```

如果启用重排序功能：
- 构建查询-文档对
- 使用专门的rerank模型进行精确相关性评分
- 按照新的得分重新排序

### 2.3 答案生成流程

#### 步骤1：构建RAG提示词
```python
BASIC_QA_TEMPLATE = '''现在的时间是{#TIME#}。你是一个专家，你擅长回答用户提问，帮我结合给定的资料，回答下面的问题。
如果问题无法从资料中获得，或无法从资料中进行回答，请回答无法回答。如果提问不符合逻辑，请回答无法回答。
如果问题可以从资料中获得，则请逐步回答。

资料：
{#RELATED_DOCUMENT#}

问题：{#QUESTION#}
'''
```

**提示词构建过程：**
1. 将检索到的相关文档拼接作为背景资料
2. 插入当前时间信息
3. 插入用户的原始问题
4. 设置明确的回答规则和格式要求

#### 步骤2：LLM生成回答
```python
def chat(self, messages: List[Dict], top_p: float, temperature: float):
    completion = self.client.chat.completions.create(
        model=self.llm_model,
        messages=messages,
        top_p=top_p,
        temperature=temperature
    )
    return completion.choices[0].message
```

**生成参数配置：**
- 模型：GLM-4-air
- temperature：0.1（低随机性，确保答案准确性）  
- top_p：1.0
- max_tokens：1024

## 3. 关键配置参数

### 3.1 模型配置
```yaml
models:
  embedding_model:
    bge-small-zh-v1.5:
      dims: 512
  rerank_model:
    bge-reranker-base
  llm:
    glm-4-air
```

### 3.2 RAG参数优化
```yaml
rag:
  chunk_size: 256        # 文档分块大小
  chunk_overlap: 20      # 分块重叠长度
  chunk_candidate: 10    # 候选文档块数量
  use_embedding: true    # 启用向量检索
  use_rerank: false      # 重排序开关
  use_rrf: true          # 启用结果融合
```

## 4. 性能优化策略

### 4.1 检索优化
- **混合检索**：结合BM25全文检索和向量检索的优势
- **RRF融合**：有效平衡不同检索策略的结果
- **分块策略**：合理的chunk大小平衡检索精度和召回率

### 4.2 向量优化  
- **向量归一化**：提高检索准确性
- **HNSW索引**：ES中使用高效的向量索引算法
- **维度优化**：512维向量在性能和效果间达到平衡

### 4.3 生成优化
- **提示词工程**：明确的指令和格式要求
- **温度控制**：低temperature确保答案稳定性
- **长度控制**：限制生成长度避免冗余

## 5. 系统特点分析

### 5.1 优势
1. **混合检索**：结合了关键词匹配和语义理解
2. **分层存储**：关系数据库+向量数据库的合理分工
3. **可扩展性**：支持多知识库管理
4. **模块化设计**：各组件职责明确，便于维护

### 5.2 可改进空间
1. **重排序功能**：当前未启用，可能影响检索精度
2. **查询优化**：缺少查询改写和意图理解
3. **多模态支持**：目前仅支持文本，可扩展图表处理
4. **缓存机制**：可添加检索结果缓存提升性能

## 6. 总结

这个RAG系统实现了从文档处理、向量化存储、混合检索到答案生成的完整流程。通过合理的架构设计和参数调优，能够为政企场景提供准确、可靠的问答服务。系统的模块化设计也为后续的功能扩展和优化提供了良好的基础。
