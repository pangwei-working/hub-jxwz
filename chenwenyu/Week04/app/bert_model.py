import os
import joblib
import numpy as np
import pandas as pd

import torch
from torch.utils.data import Dataset, DataLoader

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

from transformers import BertTokenizer
from transformers import BertForSequenceClassification

# è‡ªå®šä¹‰æ•°æ®é›†ç±»ï¼Œç»§æ‰¿è‡ªPyTorchçš„Dataset
# ç”¨äºå¤„ç†ç¼–ç åçš„æ•°æ®å’Œæ ‡ç­¾ï¼Œæ–¹ä¾¿åç»­æ‰¹é‡è¯»å–
class NewDataset(Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    # è·å–å•ä¸ªæ ·æœ¬çš„æ–¹æ³•
    def __getitem__(self, idx):
        # ä»ç¼–ç å­—å…¸ä¸­æå–input_ids, token_types_ids,attention_maskï¼Œå¹¶è½¬æ¢ä¸ºPyTorchå¼ é‡
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        # æ·»åŠ æ ‡ç­¾ï¼Œå¹¶è½¬æ¢ä¸ºå¼ é‡
        item['labels'] = torch.tensor(int(self.labels[idx]))
        return item

    # è¿”å›æ•°æ®é›†æ€»æ ·æœ¬æ•°çš„æ–¹æ³•
    def __len__(self):
        return len(self.labels)

# å®šä¹‰ç²¾åº¦è®¡ç®—å‡½æ•°
def flat_accuracy(preds, labels):
    # è·å–é¢„æµ‹ç»“æœçš„æœ€é«˜æ¦‚ç‡ç´¢å¼•
    pred_flat = np.argmax(preds, axis=1).flatten()
    # å±•å¹³çœŸå®æ ‡ç­¾
    labels_flat = labels.flatten()
    # è®¡ç®—å‡†ç¡®ç‡
    return np.sum(pred_flat == labels_flat) / len(labels_flat)

#ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹
def save_model(model, tokenizer, lbl, epoch, base_dir="./app/models"):
    """ä¿å­˜æ¨¡å‹å’Œç›¸å…³æ–‡ä»¶"""
    output_dir = f"{base_dir}/bert-finetuned-epoch{epoch}"
    os.makedirs(output_dir, exist_ok=True)
        
    # ä¿å­˜æ¨¡å‹å’Œåˆ†è¯å™¨
    model.save_pretrained(output_dir)
    tokenizer.save_pretrained(output_dir)
        
    # ä¿å­˜æ ‡ç­¾ç¼–ç å™¨
    joblib.dump(lbl, os.path.join(output_dir, "label_encoder.pkl"))
        
    print(f"Model saved to {output_dir}")
    return output_dir

# -------------------------- ä¸»è®­ç»ƒå‡½æ•° --------------------------
def main_train():
    # -------------------------- 1. æ•°æ®å‡†å¤‡ --------------------------
    # åŠ è½½æ•°æ®é›†ï¼ŒæŒ‡å®šåˆ†éš”ç¬¦ä¸ºåˆ¶è¡¨ç¬¦ï¼Œç¬¬ä¸€è¡Œä¸ºåˆ—å
    dataset = pd.read_csv("./assets/waimai_10k.csv", sep=",")
    dataset.columns = dataset.columns.str.strip()  # å»é™¤åˆ—åä¸¤ç«¯çš„ç©ºæ ¼

    print("Labelæ•°æ®ç±»å‹:", type(dataset['label'])) #pandas.core.series.Series
    print("Labelæ•°æ®ç±»å‹:", type(dataset['label'].values))    #numpy.ndarray
    print("reviewæ•°æ®ç±»å‹:", type(dataset['review'].iloc[:5]))  #pandas.core.series.Series
    print(dataset.head(5))
    
    dataset_shuffled = dataset.sample(frac=1, random_state=42).reset_index(drop=True)
    subset = dataset_shuffled.iloc[:500]

    # åˆå§‹åŒ–å¹¶æ‹Ÿåˆæ ‡ç­¾ç¼–ç å™¨ï¼Œå°†æ–‡æœ¬æ ‡ç­¾è½¬æ¢ä¸ºæ•°å­—æ ‡ç­¾ï¼ˆå¦‚0, 1, 2...ï¼‰
    lbl = LabelEncoder()
    labels = lbl.fit_transform(subset['label']) #Transform labels to normalized encoding
    unique, counts = np.unique(labels, return_counts=True)
    print(dict(zip(unique, counts)))

    # å°†æ•°æ®æŒ‰8:2çš„æ¯”ä¾‹åˆ†å‰²ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†
    # stratify å‚æ•°ç¡®ä¿è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸­å„ç±»åˆ«çš„æ ·æœ¬æ¯”ä¾‹ä¸åŸå§‹æ•°æ®é›†ä¿æŒä¸€è‡´
    x_train, x_test, train_label, test_label = train_test_split(
        list(subset['review']),
        labels,
        test_size=0.2,
        stratify=labels,
        random_state=42
    )

    # åŠ è½½BERTé¢„è®­ç»ƒçš„åˆ†è¯å™¨ï¼ˆTokenizerï¼‰
    # åˆ†è¯å™¨è´Ÿè´£å°†æ–‡æœ¬è½¬æ¢ä¸ºæ¨¡å‹å¯è¯†åˆ«çš„è¾“å…¥IDã€æ³¨æ„åŠ›æ©ç ç­‰
    tokenizer = BertTokenizer.from_pretrained('./assets/models/google-bert/bert-base-chinese')
    # æ‰“å° tokenizer åŸºæœ¬ä¿¡æ¯
    print("=== Tokenizer åŸºæœ¬ä¿¡æ¯ ===")
    print(f"Tokenizer ç±»å‹: {type(tokenizer)}")
    print(f"è¯æ±‡è¡¨å¤§å°: {tokenizer.vocab_size}")
    print(f"æ¨¡å‹æœ€å¤§é•¿åº¦: {tokenizer.model_max_length}")

    # å¯¹è®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„æ–‡æœ¬è¿›è¡Œç¼–ç 
    # truncation=Trueï¼šå¦‚æœå¥å­é•¿åº¦è¶…è¿‡max_lengthï¼Œåˆ™æˆªæ–­
    # padding=Trueï¼šå°†æ‰€æœ‰å¥å­å¡«å……åˆ°max_length
    # max_length=64ï¼šæœ€å¤§åºåˆ—é•¿åº¦
    train_encoding = tokenizer(x_train, truncation=True, padding=True, max_length=64)
    print("æ‰€æœ‰é”®:", list(train_encoding.keys()))

    for key, value in train_encoding.items():
        print(f"=== {key} ===")
        print(f"æ•°æ®ç±»å‹: {type(value)}")
        print(f"æ•°æ®å½¢çŠ¶: {len(value)} Ã— {len(value[0])}")
        print(f"ç¤ºä¾‹æ•°æ®: {value[0]}")
        print()

    test_encoding = tokenizer(x_test, truncation=True, padding=True, max_length=64)

    # -------------------------- 2. æ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨ --------------------------
    # å®ä¾‹åŒ–è‡ªå®šä¹‰æ•°æ®é›†
    train_dataset = NewDataset(train_encoding, train_label) # å•ä¸ªæ ·æœ¬è¯»å–çš„æ•°æ®é›†
    test_dataset = NewDataset(test_encoding, test_label)

    #import pdb; pdb.set_trace()

    # ä½¿ç”¨DataLoaderåˆ›å»ºæ‰¹é‡æ•°æ®åŠ è½½å™¨
    # batch_size=16ï¼šæ¯ä¸ªæ‰¹æ¬¡åŒ…å«16ä¸ªæ ·æœ¬
    # shuffle=Trueï¼šåœ¨æ¯ä¸ªepochå¼€å§‹æ—¶æ‰“ä¹±æ•°æ®ï¼Œä»¥æé«˜æ¨¡å‹æ³›åŒ–èƒ½åŠ›
    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True) # æ‰¹é‡è¯»å–æ ·æœ¬
    test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=True)

    for i, batch_data in enumerate(train_loader, 1):  # 1è¡¨ç¤ºä»1å¼€å§‹è®¡æ•°
        print(f"batch_data[{i}]æ‰€æœ‰é”®:", list(batch_data.keys()))
        print(f"batch{i} len= {len(batch_data)}")
        if i>=2:
            break
    # -------------------------- 3. æ¨¡å‹å’Œä¼˜åŒ–å™¨ --------------------------
    # åŠ è½½BERTç”¨äºåºåˆ—åˆ†ç±»çš„é¢„è®­ç»ƒæ¨¡å‹
    # num_labels=12ï¼šæŒ‡å®šåˆ†ç±»ä»»åŠ¡çš„ç±»åˆ«æ•°é‡
    # https://huggingface.co/docs/transformers/v4.56.0/en/model_doc/bert#transformers.BertForSequenceClassification
    model = BertForSequenceClassification.from_pretrained('./assets/models/google-bert/bert-base-chinese', num_labels=17)

    # è®¾ç½®è®¾å¤‡ï¼Œä¼˜å…ˆä½¿ç”¨CUDAï¼ˆGPUï¼‰ï¼Œå¦åˆ™ä½¿ç”¨CPU
    device: torch.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")

    # å°†æ¨¡å‹ç§»åŠ¨åˆ°æŒ‡å®šçš„è®¾å¤‡ä¸Š
    model.to(device)    #type: ignore

    # å®šä¹‰ä¼˜åŒ–å™¨ï¼Œä½¿ç”¨AdamWï¼Œlræ˜¯å­¦ä¹ ç‡
    optim = torch.optim.AdamW(model.parameters(), lr=2e-5)

    # -------------------------- 4.è®­ç»ƒå’ŒéªŒè¯å‡½æ•° --------------------------
    # å®šä¹‰è®­ç»ƒå‡½æ•°
    def train():
        # è®¾ç½®æ¨¡å‹ä¸ºè®­ç»ƒæ¨¡å¼
        model.train()
        total_train_loss = 0
        iter_num = 0
        total_iter = len(train_loader)

        # éå†è®­ç»ƒæ•°æ®åŠ è½½å™¨
        for batch in train_loader:
            # æ¸…é™¤ä¸Šä¸€è½®çš„æ¢¯åº¦
            optim.zero_grad()

            # å°†æ‰¹æ¬¡æ•°æ®ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)

            # æ‰§è¡Œå‰å‘ä¼ æ’­ï¼Œå¾—åˆ°æŸå¤±å’Œlogits
            outputs = model(input_ids, attention_mask=attention_mask, labels=labels) # è‡ªåŠ¨è®¡ç®—æŸå¤±
            loss = outputs[0]
            total_train_loss += loss.item()

            # åå‘ä¼ æ’­è®¡ç®—æ¢¯åº¦
            loss.backward()
            # æ¢¯åº¦è£å‰ªï¼Œé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)

            # æ›´æ–°æ¨¡å‹å‚æ•°
            optim.step()

            iter_num += 1
            # æ¯100æ­¥æ‰“å°ä¸€æ¬¡è®­ç»ƒè¿›åº¦
            if (iter_num % 100 == 0):
                print("epoth: %d, iter_num: %d, loss: %.4f, %.2f%%" % (
                    epoch, iter_num, loss.item(), iter_num / total_iter * 100))

        # æ‰“å°å¹³å‡è®­ç»ƒæŸå¤±
        print("Epoch: %d, Average training loss: %.4f" % (epoch, total_train_loss / len(train_loader)))


    # å®šä¹‰éªŒè¯å‡½æ•°
    def validation():
        # è®¾ç½®æ¨¡å‹ä¸ºè¯„ä¼°æ¨¡å¼
        model.eval()
        total_eval_accuracy = 0
        total_eval_loss = 0

        # éå†æµ‹è¯•æ•°æ®åŠ è½½å™¨
        for batch in test_dataloader:
            # åœ¨éªŒè¯é˜¶æ®µï¼Œä¸è®¡ç®—æ¢¯åº¦
            with torch.no_grad():
                # å°†æ‰¹æ¬¡æ•°æ®ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡
                input_ids = batch['input_ids'].to(device)
                attention_mask = batch['attention_mask'].to(device)
                labels = batch['labels'].to(device)

                # æ‰§è¡Œå‰å‘ä¼ æ’­
                outputs = model(input_ids, attention_mask=attention_mask, labels=labels)

            loss = outputs[0]
            logits = outputs[1]

            total_eval_loss += loss.item()
            # å°†logitså’Œæ ‡ç­¾ä»GPUç§»åŠ¨åˆ°CPUï¼Œå¹¶è½¬æ¢ä¸ºnumpyæ•°ç»„
            logits = logits.detach().cpu().numpy()
            label_ids = labels.to('cpu').numpy()
            total_eval_accuracy += flat_accuracy(logits, label_ids)

        # è®¡ç®—å¹³å‡å‡†ç¡®ç‡
        avg_loss=total_eval_loss/len(test_dataloader)
        avg_val_accuracy = total_eval_accuracy / len(test_dataloader)
        print("Accuracy: %.4f" % (avg_val_accuracy))
        print("Average testing loss: %.4f" % (avg_loss))
        print("-------------------------------")
        return avg_loss, avg_val_accuracy
    
    # -------------------------- 5. ä¸»è®­ç»ƒå¾ªç¯ --------------------------
    # å¾ªç¯è®­ç»ƒ4ä¸ªepoch
    best_accuracy=0
    for epoch in range(4):
        print("------------Epoch: %d ----------------" % epoch)
        # è®­ç»ƒæ¨¡å‹
        train()
        # éªŒè¯æ¨¡å‹
        val_loss, val_accuracy=validation()
        if val_accuracy > best_accuracy:
            best_accuracy = val_accuracy
            save_model(model, tokenizer, lbl, epoch)
            print(f"ğŸ‰ æ–°çš„æœ€ä½³æ¨¡å‹ä¿å­˜ï¼Œå‡†ç¡®ç‡: {val_accuracy:.4f}")
    print(f"\nè®­ç»ƒå®Œæˆï¼æœ€ä½³å‡†ç¡®ç‡: {best_accuracy:.4f}")


# å¦‚æœæ˜¯ç›´æ¥è¿è¡Œè¿™ä¸ªæ–‡ä»¶ï¼Œåˆ™æ‰§è¡Œè®­ç»ƒ
if __name__ == "__main__":
    main_train()