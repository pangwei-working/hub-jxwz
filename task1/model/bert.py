import pandas as pd
import torch
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from transformers import (
    BertTokenizer,
    BertForSequenceClassification,
    Trainer,
    TrainingArguments
)
from datasets import Dataset


def model_for_bert(
        data_path="D:/nlp20/week4/task1/assets/dataset/waimai_10k.csv",
        model_path="E:/models/google-bert/bert-base-chines",
        num_train_epochs=6,
        per_device_batch_size=8,
        max_seq_length=64,
        test_size=0.2
):
    """
    åŠ è½½å¤–å–è¯„ä»·æ•°æ®é›†ï¼Œè®­ç»ƒBERTæ–‡æœ¬åˆ†ç±»æ¨¡å‹ï¼Œå¹¶è¿”å›æ ¸å¿ƒå·¥å…·ï¼ˆåˆ†è¯å™¨ã€è®­ç»ƒåæ¨¡å‹ã€æ ‡ç­¾ç¼–ç å™¨ï¼‰
    """
    # -------------------------- 1. æ•°æ®åŠ è½½ä¸é¢„å¤„ç† --------------------------
    print("=" * 50)
    print("å¼€å§‹åŠ è½½å¹¶é¢„å¤„ç†æ•°æ®é›†...")
    try:
        dataset_df = pd.read_csv(data_path, sep=",")
        print(f"æˆåŠŸåŠ è½½æ•°æ®é›†ï¼Œæ€»è¡Œæ•°ï¼š{len(dataset_df)}")
    except FileNotFoundError:
        raise FileNotFoundError(f"æ•°æ®é›†æ–‡ä»¶æœªæ‰¾åˆ°ï¼Œè¯·æ£€æŸ¥è·¯å¾„ï¼š{data_path}")

    # æå–æ–‡æœ¬å’Œæ ‡ç­¾ï¼ˆå‰100æ¡æ ·æœ¬ï¼‰
    texts = list(dataset_df["review"].values[:100])
    raw_labels = dataset_df["label"].values[:100]

    # æ ‡ç­¾ç¼–ç 
    label_encoder = LabelEncoder()
    num_labels = label_encoder.fit_transform(raw_labels)

    # åˆ†å‰²è®­ç»ƒé›†ä¸æµ‹è¯•é›†
    x_train, x_test, train_labels, test_labels = train_test_split(
        texts,
        num_labels,
        test_size=test_size,
        stratify=num_labels,
        random_state=42
    )

    # æ‰“å°æ•°æ®åˆ†å¸ƒä¿¡æ¯
    num_actual_labels = len(label_encoder.classes_)
    print(f"\næ•°æ®é›†å®é™…ç±»åˆ«æ•°ï¼š{num_actual_labels}")
    print(f"ç±»åˆ«æ˜ å°„å…³ç³»ï¼š{dict(zip(range(num_actual_labels), label_encoder.classes_))}")
    print(f"è®­ç»ƒé›†æ ·æœ¬æ•°ï¼š{len(x_train)}ï¼Œæ ‡ç­¾åˆ†å¸ƒï¼š{np.bincount(train_labels)}")
    print(f"æµ‹è¯•é›†æ ·æœ¬æ•°ï¼š{len(x_test)}ï¼Œæ ‡ç­¾åˆ†å¸ƒï¼š{np.bincount(test_labels)}")
    print("æ•°æ®é›†é¢„å¤„ç†å®Œæˆï¼")
    print("=" * 50)

    # -------------------------- 2. åŠ è½½BERTåˆ†è¯å™¨ä¸é¢„è®­ç»ƒæ¨¡å‹ --------------------------
    print("\nå¼€å§‹åŠ è½½BERTåˆ†è¯å™¨ä¸é¢„è®­ç»ƒæ¨¡å‹...")
    tokenizer = BertTokenizer.from_pretrained(model_path)
    model = BertForSequenceClassification.from_pretrained(
        model_path,
        num_labels=num_actual_labels,
        ignore_mismatched_sizes=True
    )
    print(f"æˆåŠŸåŠ è½½é¢„è®­ç»ƒæ¨¡å‹ï¼š{model_path}")
    print(f"æ¨¡å‹è¾“å‡ºç±»åˆ«æ•°ï¼š{model.num_labels}ï¼ˆä¸æ•°æ®é›†ç±»åˆ«æ•°ä¸€è‡´ï¼‰")
    print("=" * 50)

    # -------------------------- 3. æ–‡æœ¬ç¼–ç ï¼ˆæ ¸å¿ƒä¿®æ”¹ï¼šè°ƒæ•´æ ‡ç­¾æ•°æ®ç±»å‹ï¼‰ --------------------------
    print("\nå¼€å§‹å¯¹æ–‡æœ¬æ•°æ®è¿›è¡Œç¼–ç ï¼ˆTokenizeï¼‰...")
    # è®­ç»ƒé›†ç¼–ç 
    train_encodings = tokenizer(
        x_train,
        truncation=True,
        padding="max_length",
        max_length=max_seq_length,
        return_tensors="pt"
    )
    # æµ‹è¯•é›†ç¼–ç 
    test_encodings = tokenizer(
        x_test,
        truncation=True,
        padding="max_length",
        max_length=max_seq_length,
        return_tensors="pt"
    )

    # è½¬æ¢ä¸ºHugging Face Datasetæ ¼å¼
    # æ ¸å¿ƒä¿®å¤ï¼šå•ç±»åˆ«æ—¶ä½¿ç”¨torch.float32ç±»å‹ï¼Œå¤šç±»åˆ«æ—¶ä½¿ç”¨torch.long
    label_dtype = torch.float32 if num_actual_labels == 1 else torch.long

    train_dataset = Dataset.from_dict({
        "input_ids": train_encodings["input_ids"],
        "attention_mask": train_encodings["attention_mask"],
        "labels": torch.tensor(train_labels, dtype=label_dtype)  # ä¿®å¤æ•°æ®ç±»å‹
    })
    test_dataset = Dataset.from_dict({
        "input_ids": test_encodings["input_ids"],
        "attention_mask": test_encodings["attention_mask"],
        "labels": torch.tensor(test_labels, dtype=label_dtype)  # ä¿®å¤æ•°æ®ç±»å‹
    })
    print(f"è®­ç»ƒé›†ç¼–ç å®Œæˆï¼Œæ ·æœ¬æ ¼å¼ï¼š{train_dataset[0].keys()}")
    print(f"æ ‡ç­¾æ•°æ®ç±»å‹ï¼š{label_dtype}ï¼ˆè‡ªåŠ¨é€‚é…ç±»åˆ«æ•°ï¼‰")
    print(f"æµ‹è¯•é›†ç¼–ç å®Œæˆï¼Œæ ·æœ¬æ•°é‡ï¼š{len(test_dataset)}")
    print("=" * 50)

    # -------------------------- 4. å®šä¹‰è¯„ä¼°æŒ‡æ ‡ï¼ˆå‡†ç¡®ç‡ï¼‰ --------------------------
    def compute_metrics(eval_pred):
        logits, labels = eval_pred
        # å•ç±»åˆ«æ—¶éœ€è¦ç‰¹æ®Šå¤„ç†ï¼ˆsigmoidæ¿€æ´»ï¼‰
        if num_actual_labels == 1:
            predictions = (logits > 0.5).astype(int).flatten()  # å¤§äº0.5è§†ä¸ºæ­£ä¾‹
        else:
            predictions = np.argmax(logits, axis=-1)
        accuracy = (predictions == labels).mean()
        return {"accuracy": round(accuracy, 4)}

    # -------------------------- 5. é…ç½®è®­ç»ƒå‚æ•° --------------------------
    print("\né…ç½®è®­ç»ƒå‚æ•°...")
    training_args = TrainingArguments(
        output_dir="./results",
        num_train_epochs=num_train_epochs,
        per_device_train_batch_size=per_device_batch_size,
        per_device_eval_batch_size=per_device_batch_size,
        warmup_steps=100,
        weight_decay=0.01,
        logging_dir="./logs",
        logging_steps=10,
        eval_strategy="epoch",
        save_strategy="epoch",
        save_total_limit=3,
        load_best_model_at_end=True,
        metric_for_best_model="eval_accuracy",
        fp16=torch.cuda.is_available(),
        disable_tqdm=False
    )
    print("è®­ç»ƒå‚æ•°é…ç½®å®Œæˆï¼Œå³å°†å¼€å§‹è®­ç»ƒï¼")
    print("=" * 50)

    # -------------------------- 6. åˆå§‹åŒ–Trainerå¹¶å¼€å§‹è®­ç»ƒ --------------------------
    print("\nå¼€å§‹è®­ç»ƒæ¨¡å‹...")
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=test_dataset,
        compute_metrics=compute_metrics
    )

    # å¯åŠ¨è®­ç»ƒ
    trainer.train()

    # -------------------------- 7. æœ€ç»ˆè¯„ä¼°ä¸æ¨¡å‹è¿”å› --------------------------
    print("\nè®­ç»ƒå®Œæˆï¼å¼€å§‹æœ€ç»ˆæµ‹è¯•é›†è¯„ä¼°...")
    final_eval_results = trainer.evaluate()
    print(f"\næœ€ç»ˆè¯„ä¼°ç»“æœï¼š")
    print(f"æµ‹è¯•é›†æŸå¤±ï¼ˆeval_lossï¼‰ï¼š{round(final_eval_results['eval_loss'], 4)}")
    print(f"æµ‹è¯•é›†å‡†ç¡®ç‡ï¼ˆeval_accuracyï¼‰ï¼š{round(final_eval_results['eval_accuracy'], 4)}")
    print("=" * 50)

    return tokenizer, model, label_encoder


if __name__ == "__main__":
    try:
        tokenizer, trained_model, label_encoder = model_for_bert()
        print("\nâœ… æ¨¡å‹è®­ç»ƒå…¨æµç¨‹å®Œæˆï¼")
        print(f"ğŸ”§ å¯ç”¨äºæ¨ç†çš„å·¥å…·ï¼š")
        print(f"   - åˆ†è¯å™¨ï¼š{type(tokenizer).__name__}")
        print(f"   - è®­ç»ƒåæ¨¡å‹ï¼š{type(trained_model).__name__}")
        print(f"   - æ ‡ç­¾ç¼–ç å™¨ï¼šç±»åˆ«æ˜ å°„ {dict(zip(range(len(label_encoder.classes_)), label_encoder.classes_))}")
    except Exception as e:
        print(f"\nâŒ è®­ç»ƒè¿‡ç¨‹å‡ºé”™ï¼š{str(e)}")